{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pipelines.video_action_recognition_config import get_cfg_defaults\n",
    "# from models.tuber_ava import build_model\n",
    "from glob import glob\n",
    "import json\n",
    "import datasets.video_transforms as T\n",
    "import random\n",
    "\n",
    "def read_label_map(label_map_path):\n",
    "\n",
    "    item_id = None\n",
    "    item_name = None\n",
    "    items = {}\n",
    "    \n",
    "    with open(label_map_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line.replace(\" \", \"\")\n",
    "            if line == \"item{\":\n",
    "                pass\n",
    "            elif line == \"}\":\n",
    "                pass\n",
    "            elif \"id:\" in line:\n",
    "                item_id = int(line.split(\":\", 1)[1].strip())\n",
    "            elif \"name\" in line:\n",
    "                item_name = line.split(\":\", 1)[1].replace(\"'\", \"\").strip()\n",
    "\n",
    "            if item_id is not None and item_name is not None:\n",
    "                items[item_id] = item_name\n",
    "                item_id = None\n",
    "                item_name = None\n",
    "            items[81] = \"happens\"\n",
    "\n",
    "    return items\n",
    "\n",
    "items = read_label_map(\"../assets/ava_action_list_v2.1.pbtxt\")\n",
    "\n",
    "def make_transforms(image_set, cfg):\n",
    "    normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    print(\"transform image crop: {}\".format(cfg.CONFIG.DATA.IMG_SIZE))\n",
    "    if image_set == 'train':\n",
    "        return T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomSizeCrop_Custom(cfg.CONFIG.DATA.IMG_SIZE),\n",
    "            T.ColorJitter(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    if image_set == 'val':\n",
    "        return T.Compose([\n",
    "            T.Resize_Custom(cfg.CONFIG.DATA.IMG_SIZE),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    if image_set == 'visual':\n",
    "        return T.Compose([\n",
    "            T.Resize_Custom(cfg.CONFIG.DATA.IMG_SIZE),\n",
    "            normalize,\n",
    "        ])\n",
    "    raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "# cfg = get_cfg_defaults()\n",
    "# cfg.merge_from_file(\"./configuration/TubeR_CSN50_AVA21.yaml\")\n",
    "# model, _, _ = build_model(cfg)\n",
    "\n",
    "from models.dab_conv_trans import build_model\n",
    "cfg2 = get_cfg_defaults()\n",
    "cfg2.merge_from_file(\"./configuration/Dab_conv_trans_CSN50_AVA22.yaml\")\n",
    "current_model, _, _ = build_model(cfg2)\n",
    "\n",
    "from models.dab_baseline import build_model\n",
    "cfg3 = get_cfg_defaults()\n",
    "cfg3.merge_from_file(\"./configuration/Dab_hier_CSN50_AVA22.yaml\")\n",
    "baseline_model, _, _ = build_model(cfg3)\n",
    "\n",
    "# og_checkpoint = torch.load(\"../pretrained_models/main/tuber.pth\")\n",
    "# my_checkpoint = torch.load(\"../pretrained_models/main/dab_hoper.pth\")\n",
    "my_checkpoint = torch.load(\"/mnt/video_nfs4/users/jinsung/results/tubelet-transformer/AVA_Tuber/Dab_conv_trans_hada_270-245/checkpoints/ckpt_epoch_06.pth\")\n",
    "baseline_checkpoint = torch.load(\"../pretrained_models/main/baseline.pth\")\n",
    "\n",
    "# model_dict = model.state_dict()\n",
    "curr_model_dict = current_model.state_dict()\n",
    "baseline_model_dict = baseline_model.state_dict()\n",
    "\n",
    "\n",
    "# print(\"---------og------------\")\n",
    "# pretrained_dict = {k[7:]: v for k, v in og_checkpoint['model'].items() if k[7:] in model_dict}\n",
    "# unused_dict = {k[:7]: v for k, v in og_checkpoint['model'].items() if not k[7:] in model_dict}\n",
    "# not_found_dict = {k: v for k, v in model_dict.items() if not \"module.\"+k in og_checkpoint['model']}\n",
    "# print(\"# successfully loaded model layers:\", len(pretrained_dict.keys()))\n",
    "# print(\"# unused model layers:\", len(unused_dict.keys()))\n",
    "# print(\"# not found layers:\", len(not_found_dict.keys()))\n",
    "# model_dict.update(pretrained_dict)\n",
    "# model.load_state_dict(model_dict)\n",
    "\n",
    "print(\"---------mine------------\")\n",
    "pretrained_dict = {k[7:]: v for k, v in my_checkpoint['model'].items() if k[7:] in curr_model_dict}\n",
    "unused_dict = {k[7:]: v for k, v in my_checkpoint['model'].items() if not k[7:] in curr_model_dict}\n",
    "not_found_dict = {k: v for k, v in curr_model_dict.items() if not \"module\"+k in my_checkpoint['model']}\n",
    "print(\"# successfully loaded model layers:\", len(pretrained_dict.keys()))\n",
    "print(\"# unused model layers:\", len(unused_dict.keys()))\n",
    "print(\"# not found layers:\", len(not_found_dict.keys()))\n",
    "curr_model_dict.update(pretrained_dict)\n",
    "current_model.load_state_dict(curr_model_dict)\n",
    "\n",
    "print(\"---------baseline------------\")\n",
    "pretrained_dict = {k[7:]: v for k, v in baseline_checkpoint['model'].items() if k[7:] in baseline_model_dict}\n",
    "unused_dict = {k[7:]: v for k, v in baseline_checkpoint['model'].items() if not k[7:] in baseline_model_dict}\n",
    "not_found_dict = {k: v for k, v in baseline_model_dict.items() if not \"module\"+k in baseline_checkpoint['model']}\n",
    "print(\"# successfully loaded model layers:\", len(pretrained_dict.keys()))\n",
    "print(\"# unused model layers:\", len(unused_dict.keys()))\n",
    "print(\"# not found layers:\", len(not_found_dict.keys()))\n",
    "baseline_model_dict.update(pretrained_dict)\n",
    "baseline_model.load_state_dict(baseline_model_dict)\n",
    "\n",
    "print(\"--------- all models are successfully loaded ------------\")\n",
    "\n",
    "\n",
    "transforms=make_transforms(\"val\", cfg2)\n",
    "\n",
    "# model.eval()\n",
    "current_model.eval()\n",
    "baseline_model.eval()\n",
    "# sample_image1_path = \"/mnt/tmp/frames/xeGWXqSvC-8/xeGWXqSvC-8_000360.jpg\" #False\n",
    "# sample_image2_path = \"/mnt/tmp/frames/CMCPhm2L400/CMCPhm2L400_011200.jpg\" #False\n",
    "# sample_image3_path = \"/mnt/tmp/frames/Gvp-cj3bmIY/Gvp-cj3bmIY_024750.jpg\" #True\n",
    "\n",
    "# '/home/nsml/assets/ava_{}_v21.json'\n",
    "val_bbox_json = json.load(open(cfg2.CONFIG.DATA.ANNO_PATH.format(\"val\")))\n",
    "video_frame_bbox = val_bbox_json[\"video_frame_bbox\"]\n",
    "\n",
    "def sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    a: hs x bs x dim\n",
    "    b: nq x bs x dim\n",
    "    output: bs x hs x nq\n",
    "    \"\"\"\n",
    "    a, b = a.permute(1,0,2), b.permute(1,0,2)\n",
    "    a_n, b_n = a.norm(dim=2)[:, :, None], b.norm(dim=2)[:, :, None]\n",
    "    a_norm = a / torch.clamp(a_n, min=eps)\n",
    "    b_norm = b / torch.clamp(b_n, min=eps)\n",
    "    sim_mt = torch.bmm(a_norm, b_norm.transpose(1, 2))\n",
    "    return sim_mt\n",
    "\n",
    "def load_annotation(sample_id, video_frame_list): # (val 혹은 train의 key frame을 표시해놓은 list)\n",
    "\n",
    "    num_classes = 80\n",
    "    boxes, classes = [], []\n",
    "    target = {}\n",
    "\n",
    "    first_img = cv2.imread(video_frame_list[0])\n",
    "\n",
    "    oh = first_img.shape[0]\n",
    "    ow = first_img.shape[1]\n",
    "    if oh <= ow:\n",
    "        nh = 256\n",
    "        nw = 256 * (ow / oh)\n",
    "    else:\n",
    "        nw = 256\n",
    "        nh = 256 * (oh / ow)\n",
    "\n",
    "    p_t = int(32 // 2)\n",
    "    key_pos = p_t\n",
    "\n",
    "    anno_entity = video_frame_bbox[sample_id]\n",
    "\n",
    "    for i, bbox in enumerate(anno_entity[\"bboxes\"]):\n",
    "        label_tmp = np.zeros((num_classes, ))\n",
    "        acts_p = anno_entity[\"acts\"][i]\n",
    "        for l in acts_p:\n",
    "            label_tmp[l] = 1\n",
    "\n",
    "        if np.sum(label_tmp) == 0: continue\n",
    "        p_x = np.int_(bbox[0] * nw)\n",
    "        p_y = np.int_(bbox[1] * nh)\n",
    "        p_w = np.int_(bbox[2] * nw)\n",
    "        p_h = np.int_(bbox[3] * nh)\n",
    "\n",
    "        boxes.append([p_t, p_x, p_y, p_w, p_h])\n",
    "        classes.append(label_tmp)\n",
    "\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 5)\n",
    "    boxes[:, 1::3].clamp_(min=0, max=int(nw))\n",
    "    boxes[:, 2::3].clamp_(min=0, max=nh)\n",
    "\n",
    "    if boxes.shape[0]:\n",
    "        raw_boxes = F.pad(boxes, (1, 0, 0, 0), value=0)\n",
    "    else:\n",
    "        raw_boxes = boxes\n",
    "    classes = np.array(classes)\n",
    "    classes = torch.as_tensor(classes, dtype=torch.float32).reshape(-1, num_classes)\n",
    "\n",
    "    target[\"image_id\"] = [str(sample_id).replace(\",\", \"_\"), key_pos]\n",
    "    target['boxes'] = boxes\n",
    "    target['raw_boxes'] = raw_boxes\n",
    "    target[\"labels\"] = classes\n",
    "    target[\"orig_size\"] = torch.as_tensor([int(nh), int(nw)])\n",
    "    target[\"size\"] = torch.as_tensor([int(nh), int(nw)])\n",
    "    # self.index_cnt = self.index_cnt + 1\n",
    "\n",
    "    return target\n",
    "\n",
    "def loadvideo(start_img, vid, frame_key):\n",
    "    frame_path = \"/mnt/tmp/frames/{}\"\n",
    "    video_frame_path = frame_path.format(vid)\n",
    "    video_frame_list = sorted(glob(video_frame_path + '/*.jpg'))\n",
    "\n",
    "    if len(video_frame_list) == 0:\n",
    "        print(\"path doesnt exist\", video_frame_path)\n",
    "        return [], []\n",
    "    \n",
    "    target = load_annotation(frame_key, video_frame_list)\n",
    "\n",
    "    start_img = np.max(start_img, 0)\n",
    "    end_img = start_img + 32 * 2\n",
    "    indx_img = list(np.clip(range(start_img, end_img, 2), 0, len(video_frame_list) - 1))\n",
    "    buffer = []\n",
    "    for frame_idx in indx_img:\n",
    "        tmp = Image.open(video_frame_list[frame_idx])\n",
    "        tmp = tmp.resize((target['orig_size'][1], target['orig_size'][0]))\n",
    "        buffer.append(tmp)\n",
    "\n",
    "    return buffer, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TubeR's csn152, ava21 results\n",
    "plt.figure()\n",
    "%matplotlib inline\n",
    "\n",
    "go_on = True\n",
    "while go_on:\n",
    "    gpu_num = random.randint(0,7)\n",
    "\n",
    "    # detection = './tuber_res/{}.txt'.format(gpu_num) #numbers are changeable\n",
    "    # gt = './tuber_res/GT_{}.txt'.format(gpu_num)\n",
    "\n",
    "    baseline_detection = './baseline_res/{}.txt'.format(gpu_num)\n",
    "    baseline_gt = './baseline_res/GT_{}.txt'.format(gpu_num)\n",
    "\n",
    "    # my current model's results\n",
    "    my_detection = './my_res/{}.txt'.format(gpu_num) #numbers are changeable\n",
    "    my_gt = './my_res/GT_{}.txt'.format(gpu_num)\n",
    "    # my_detection = './res/{}.txt'.format(gpu_num) #numbers are changeable\n",
    "    # my_gt = './res/GT_{}.txt'.format(gpu_num)\n",
    "\n",
    "\n",
    "    # what label am I interested in?\n",
    "    # label = 54 # \"stand\", for example\n",
    "    # LOI = [12, 14, 21, 23, 25, 29, 33, 35, 37, 40, 42, 44, 45, 46, 55, 56, 59, 60, 61, 62, 63, 64, 65, 68, 72, 75, 77]\n",
    "    # label = random.sample(LOI, 1)[0]\n",
    "    label = random.randint(1,80)\n",
    "    \n",
    "    # find a video key frame with desired label\n",
    "\n",
    "    key_frame_candidates = []\n",
    "    with open(my_gt) as f:\n",
    "        for line in f.readlines():\n",
    "            img_id = line.split(' ')[0]\n",
    "            annotation = [int(float(n)) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "            multi_hot_obj_label = annotation[6:]\n",
    "            if multi_hot_obj_label[label-1] == 1:\n",
    "                key_frame_candidates.append(img_id)\n",
    "    \n",
    "    # with open(baseline_gt) as f:\n",
    "    #     for line in f.readlines():\n",
    "    #         img_id = line.split(' ')[0]\n",
    "    #         annotation = [int(float(n)) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "    #         multi_hot_obj_label = annotation[6:]\n",
    "    #         if multi_hot_obj_label[label-1] == 1:\n",
    "    #             key_frame_candidates.append(img_id)\n",
    "\n",
    "    # pick one of key_frame_candidates:\n",
    "    if len(key_frame_candidates) == 0:\n",
    "        print(\"no key frame found with following label: {}\".format(items[label]), \" try another gpu_num\")\n",
    "\n",
    "    ind = random.randint(0, len(key_frame_candidates)-1)\n",
    "    key_frame = key_frame_candidates[ind]\n",
    "    frame_second = key_frame.split(\"_\")[-1]\n",
    "    vid = \"_\".join(key_frame.split('_')[:-1])\n",
    "    \"\"\"\n",
    "    # frame_key is one of \"xeGWXqSvC-8,0911\", \"CMCPhm2L400,1274\", \"Gvp-cj3bmIY,1725\", \"Gvp-cj3bmIY_1675\"\n",
    "    frame_key = \"Gvp-cj3bmIY,1675\" \n",
    "    vid, frame_second = frame_key.split(',')\n",
    "    \"\"\"\n",
    "    frame_key = \",\".join([vid, frame_second])\n",
    "    timef = int(frame_second) - 900\n",
    "    start_img = np.max((timef * 30 - 32 // 2 * 2, 0))\n",
    "\n",
    "    imgs, target = loadvideo(start_img, vid, frame_key)\n",
    "\n",
    "    \"\"\"\n",
    "    start_img: start_img number, int\n",
    "    vid: xeGWXqSvC-8, CMCPhm2L400, Gcp-cj3bmIY\n",
    "    frame_key: 0911, 1274, 1725\n",
    "\n",
    "    \"\"\"\n",
    "    orig_vid = imgs\n",
    "    plt.imshow(orig_vid[16])\n",
    "    plt.show()\n",
    "    response = input()\n",
    "    \n",
    "    if response == \"y\":\n",
    "        go_on = False\n",
    "\n",
    "imgs, target = transforms(imgs, target)\n",
    "ho,wo = imgs[0].shape[-2], imgs[0].shape[-1]\n",
    "imgs = torch.stack(imgs, dim=0)\n",
    "imgs = imgs.permute(1, 0, 2, 3)\n",
    "    \n",
    "\n",
    "# print(len(imgs), imgs[0].shape, target)\n",
    "\n",
    "# device = \"cuda:0\"\n",
    "# model = model.to(device)\n",
    "# imgs = imgs.to(device)\n",
    "\n",
    "device2 = \"cuda:0\"\n",
    "current_model = current_model.to(device2)\n",
    "imgs2 = imgs.to(device2)\n",
    "\n",
    "device3 = \"cuda:1\"\n",
    "baseline_model = baseline_model.to(device3)\n",
    "imgs3 = imgs.to(device3)\n",
    "\n",
    "# print(attn_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out intermediate \n",
    "\n",
    "#tuber og model\n",
    "# conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
    "# enc_features = []\n",
    "# query_features = []\n",
    "# cls_enc_attn_weights, cls_dec_attn_weights = [], []\n",
    "# cls_enc_features = []\n",
    "# cls_query_features = []\n",
    "\n",
    "#current model\n",
    "curr_conv_features, curr_enc_attn_weights, curr_dec_attn_weights = [], [], []\n",
    "curr_enc_features = []\n",
    "curr_query_features = []\n",
    "curr_key_features = []\n",
    "curr_cls_enc_attn_weights, curr_cls_dec_attn_weights = [], []\n",
    "curr_cls_enc_features = []\n",
    "curr_cls_query_features = []\n",
    "curr_actor_features = []\n",
    "curr_global_features = []\n",
    "\n",
    "#baseline model\n",
    "baseline_conv_features, baseline_enc_attn_weights, baseline_dec_attn_weights = [], [], []\n",
    "baseline_enc_features = []\n",
    "baseline_query_features = []\n",
    "baseline_cls_enc_attn_weights, baseline_cls_dec_attn_weights = [], []\n",
    "baseline_cls_enc_features = []\n",
    "baseline_cls_query_features = []\n",
    "baseline_final_features = []\n",
    "\n",
    "\n",
    "# hooks = [\n",
    "#     model.backbone.body.layer4[-2].register_forward_hook(\n",
    "#         lambda self, input, output: conv_features.append(output)\n",
    "#     ),\n",
    "#     model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
    "#         lambda self, input, output: enc_attn_weights.append(output[1])\n",
    "#     ),\n",
    "#     model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
    "#         lambda self, input, output: enc_features.append(output[0])\n",
    "#     ),\n",
    "#     # model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
    "#     #     lambda self, input, output: dec_attn_weights.append(output[1])\n",
    "#     # ),\n",
    "#     model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
    "#         lambda self, input, output: query_features.append(output[0])\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# for i in range(6):\n",
    "#     hooks.append(\n",
    "#         model.transformer.decoder.layers[i].multihead_attn.register_forward_hook(\n",
    "#             lambda self, input, output: dec_attn_weights.append(output[1])\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# hooks2 = [\n",
    "#     model.encoder.layers[-1].self_attn_t.register_forward_hook(\n",
    "#         lambda self, input, output: cls_enc_attn_weights.append(output[1])\n",
    "#     ),\n",
    "#     model.encoder.layers[-1].self_attn_t.register_forward_hook(\n",
    "#         lambda self, input, output: cls_enc_features.append(output[0])\n",
    "#     ),\n",
    "#     model.cross_attn.register_forward_hook(\n",
    "#         lambda self, input, output: cls_dec_attn_weights.append(output[1])\n",
    "#     ),    \n",
    "#     model.cross_attn.register_forward_hook(\n",
    "#         lambda self, input, output: cls_query_features.append(output[0])\n",
    "#     ),    \n",
    "# ]\n",
    "\n",
    "cur_hooks = []\n",
    "cur_hooks.append(current_model.backbone.body.layer4[-2].register_forward_hook(\n",
    "        lambda self, input, output: curr_conv_features.append(output)\n",
    "        ),\n",
    "    )\n",
    "# cur_hooks.append(\n",
    "#     current_model.transformer.decoder.conv_blocks[-1].register_forward_hook(\n",
    "#         lambda self, input, output: curr_query_features.append(output)\n",
    "#     )\n",
    "# )\n",
    "cur_hooks.append(\n",
    "    current_model.transformer.decoder.cross_attn.register_forward_hook(\n",
    "        lambda self, input, output: curr_query_features.append(output[1])\n",
    "    )\n",
    ")\n",
    "# cur_hooks.append(\n",
    "#     current_model.transformer.decoder.k_proj.register_forward_hook(\n",
    "#         lambda self, input, output: curr_key_features.append(output)\n",
    "#     )\n",
    "# )\n",
    "cur_hooks.append(\n",
    "    current_model.transformer.encoder.register_forward_hook(\n",
    "        lambda self, input, output: curr_global_features.append(output)\n",
    "    )\n",
    ")\n",
    "cur_hooks.append(\n",
    "    current_model.transformer.decoder.cls_norm.register_forward_hook(\n",
    "        lambda self, input, output: curr_actor_features.append(output)\n",
    "    )\n",
    ")\n",
    "# for i in range(6):\n",
    "    # cur_hooks.append(\n",
    "    #     current_model.transformer.decoder.cls_layers[i].cross_attn.register_forward_hook(\n",
    "    #         lambda self, input, output: curr_cls_dec_attn_weights.append(output[1])\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "\n",
    "baseline_hooks = []\n",
    "baseline_hooks.append(baseline_model.backbone.body.layer4[-2].register_forward_hook(\n",
    "        lambda self, input, output: baseline_conv_features.append(output)\n",
    "        ),\n",
    "    )\n",
    "baseline_hooks.append(baseline_model.transformer.encoder.layers[-1].norm2_t.register_forward_hook(\n",
    "        lambda self, input, output: baseline_enc_features.append(output[16])\n",
    "        ),\n",
    "    )\n",
    "baseline_hooks.append(baseline_model.transformer.decoder.layers[-1].norm2.register_forward_hook(\n",
    "        lambda self, input, output: baseline_final_features.append(output)\n",
    "        ),\n",
    "    )\n",
    "for i in range(6):\n",
    "    baseline_hooks.append(\n",
    "        baseline_model.transformer.decoder.layers[i].cross_attn.register_forward_hook(\n",
    "            lambda self, input, output: baseline_dec_attn_weights.append(output[1])\n",
    "        )\n",
    "    )\n",
    "    # baseline_hooks.append(\n",
    "    #     baseline_model.transformer.decoder.cls_layers[i].cross_attn.register_forward_hook(\n",
    "    #         lambda self, input, output: baseline_cls_dec_attn_weights.append(output)\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "# outputs = model(imgs.unsqueeze(0))\n",
    "my_outputs = current_model(imgs2.unsqueeze(0))\n",
    "baseline_outputs = baseline_model(imgs3.unsqueeze(0))\n",
    "\n",
    "# for hook in hooks:\n",
    "#     hook.remove()\n",
    "# for hook in hooks2:\n",
    "#     hook.remove()\n",
    "\n",
    "for hook in cur_hooks:\n",
    "    hook.remove()\n",
    "\n",
    "for hook in baseline_hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# conv_features = conv_features[0]\n",
    "# enc_attn_weights = enc_attn_weights[0]\n",
    "# dec_attn_weights = dec_attn_weights\n",
    "# enc_features = enc_features[0]\n",
    "# query_features = query_features[0]\n",
    "# cls_enc_attn_weights = cls_enc_attn_weights[0]\n",
    "# cls_dec_attn_weights = cls_dec_attn_weights[0]\n",
    "# cls_enc_features = cls_enc_features[0]\n",
    "# cls_query_features = cls_query_features[0]\n",
    "\n",
    "curr_conv_features = curr_conv_features[0]\n",
    "curr_query_features = curr_query_features\n",
    "# curr_key_features = curr_key_features\n",
    "curr_key_features = current_model.transformer.decoder.class_queries\n",
    "# curr_dec_attn_weights = curr_dec_attn_weights\n",
    "# curr_cls_dec_attn_weights = curr_cls_dec_attn_weights\n",
    "# curr_cls_enc_features = curr_cls_enc_features[0]\n",
    "# curr_cls_query_features = curr_cls_query_features\n",
    "# curr_cls_dec_attn_weights = curr_cls_dec_attn_weights #/ len(curr_cls_dec_attn_weights)\n",
    "# curr_dec_attn_weights = curr_dec_attn_weights #/ len(curr_dec_attn_weights)\n",
    "\n",
    "baseline_conv_features = baseline_conv_features[0]\n",
    "# baseline_dec_attn_weights = baseline_dec_attn_weights\n",
    "baseline_enc_features = baseline_enc_features[0]\n",
    "baseline_final_features = baseline_final_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_vid[16]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test to see if matplotlib works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = curr_conv_features.shape[-2:]\n",
    "# attn = (curr_query_features*curr_key_features).sum(dim=1).flatten(1).softmax(dim=1).reshape(-1, 1, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_query_features[0].shape\n",
    "# current_model.transformer.decoder.q_proj.weight.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im=misc.imread(\"photosAfterAverage/exampleAfterAverage1.jpg\")\n",
    "%matplotlib inline\n",
    "h, w = curr_conv_features.shape[-2:]\n",
    "plt.figure()\n",
    "plt.imshow(orig_vid[16])\n",
    "proj_weight = current_model.transformer.decoder.q_proj.weight.squeeze()\n",
    "query = torch.matmul(curr_query_features[-1].permute(0,2,3,1), proj_weight)\n",
    "# query = curr_query_features[-1]\n",
    "curr_key_features_ = curr_key_features[None, :, :, None, None].expand(15, -1, -1, h, w)\n",
    "attn = (query.permute(0,3,1,2)[:, None].expand(-1, 80, -1, -1, -1) * curr_key_features_).sum(dim=2).flatten(2).softmax(dim=2).reshape(15, -1, h, w)[:, :, None]\n",
    "# attn = (query[:, None].expand(-1, 80, -1, -1, -1) * curr_key_features_).sum(dim=2).flatten(2).softmax(dim=2).reshape(15, -1, h, w)[:, :, None]\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.imshow(attn[6, 53].detach().cpu().view(h, w), cmap='seismic', interpolation='bicubic', alpha=.6, extent=(xmin, xmax, ymin, ymax))\n",
    "# plt.imshow(curr_query_features[0][0, 2].detach().cpu().view(h, w), cmap='copper', interpolation='bicubic', alpha=.6, extent=(xmin, xmax, ymin, ymax))\n",
    "# plt.imshow(cls_dec_attn_weights[0, 2, :].detach().cpu().view(h, w), cmap='copper', interpolation='bicubic', alpha=.7, extent=(xmin, xmax, ymin, ymax))\n",
    "\n",
    "# plt.imshow(test.detach().cpu().view(h, w), cmap='copper', interpolation='nearest', alpha=.8, extent=(xmin, xmax, ymin, ymax))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = curr_conv_features.shape[-2:]\n",
    "attn = []\n",
    "for i in range(6):\n",
    "    query = torch.matmul(curr_query_features[i].permute(0,2,3,1), proj_weight)\n",
    "    # query = curr_query_features[i]\n",
    "    attn_ = (query.permute(0,3,1,2)[:, None].expand(-1, 80, -1, -1, -1) * curr_key_features_).sum(dim=2).flatten(2).softmax(dim=2).reshape(15, -1, h, w)[:, :, None]\n",
    "    # attn_ = (query[:, None].expand(-1, 80, -1, -1, -1) * curr_key_features_).sum(dim=2).flatten(2).softmax(dim=2).reshape(15, -1, h, w)[:, :, None]\n",
    "    # attn_ = (curr_query_features[i][:, None].expand(-1, 80, -1, -1, -1) * curr_key_features_).sum(dim=2).flatten(2).softmax(dim=2).reshape(15, -1, h, w)[:, :, None]\n",
    "    attn.append(attn_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(orig_vid[16])\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "baseline_enc_features\n",
    "plt.imshow(baseline_enc_features.norm(dim=1).detach().cpu().view(h, w), cmap='seismic', interpolation='bicubic', alpha=.6, extent=(xmin, xmax, ymin, ymax))\n",
    "# plt.imshow(cls_dec_attn_weights[0, 2, :].detach().cpu().view(h, w), cmap='copper', interpolation='bicubic', alpha=.7, extent=(xmin, xmax, ymin, ymax))\n",
    "\n",
    "# plt.imshow(test.detach().cpu().view(h, w), cmap='copper', interpolation='nearest', alpha=.8, extent=(xmin, xmax, ymin, ymax))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load annotation files\n",
    "#### TODO: load it with listfiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_key is one of \"xeGWXqSvC-8,0911\", \"CMCPhm2L400,1274\", \"Gvp-cj3bmIY,1725\" \n",
    "# query_logits = []\n",
    "# with open(detection) as f:\n",
    "#     for line in f.readlines():\n",
    "#         img_id = line.split(' ')[0]\n",
    "#         if key_frame != img_id:\n",
    "#             continue\n",
    "#         else:\n",
    "#             annotation = [float(n) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "#             # multi_hot_obj_label = [int(n) for n in annotation[6:]]\n",
    "#             query_logits.append(annotation)\n",
    "\n",
    "# anno_dict = {}\n",
    "# with open(gt) as f:\n",
    "#     for line in f.readlines():\n",
    "#         img_id = line.split(' ')[0]\n",
    "#         if img_id != key_frame:\n",
    "#             continue\n",
    "#         else:\n",
    "#             annotation = [int(float(n)) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "#             gt_coord = annotation[2:6]\n",
    "#             # gtxmin, gtymin, gtxmax, gtymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "#             gt_multi_hot_label = annotation[6:]\n",
    "#             gt_cat = [items[i+1] for i, e in enumerate(gt_multi_hot_label) if e]\n",
    "#             if img_id not in anno_dict.keys():\n",
    "#                 anno_dict[img_id] = {\n",
    "#                     \"obj\": [gt_cat],\n",
    "#                     \"coord\": [gt_coord]\n",
    "#                 }           \n",
    "#             else:\n",
    "#                 anno_dict[img_id][\"obj\"].append(gt_cat)\n",
    "#                 anno_dict[img_id][\"coord\"].append(gt_coord) \n",
    "\n",
    "my_detection = './my_res/{}.txt'\n",
    "my_gt = './my_res/GT_{}.txt'\n",
    "\n",
    "# my_detection = './res/{}.txt'\n",
    "# my_gt = './res/GT_{}.txt'\n",
    "\n",
    "\n",
    "my_query_logits = []\n",
    "\n",
    "for i in range(8):\n",
    "    my_detection_ = my_detection.format(i)\n",
    "    with open(my_detection_) as f:\n",
    "        for line in f.readlines():\n",
    "            img_id = line.split(' ')[0]\n",
    "            if key_frame != img_id:\n",
    "                continue\n",
    "            else:\n",
    "                annotation = [float(n) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "                # multi_hot_obj_label = [int(n) for n in annotation[6:]]\n",
    "                my_query_logits.append(annotation)\n",
    "\n",
    "my_anno_dict = {}\n",
    "for j in range(8):\n",
    "    my_gt_ = my_gt.format(j)\n",
    "    with open(my_gt_) as f:\n",
    "        for line in f.readlines():\n",
    "            img_id = line.split(' ')[0]\n",
    "            if img_id != key_frame:\n",
    "                continue\n",
    "            else:\n",
    "                annotation = [int(float(n)) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "                gt_coord = annotation[2:6]\n",
    "                # gtxmin, gtymin, gtxmax, gtymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "                gt_multi_hot_label = annotation[6:]\n",
    "                gt_cat = [items[i+1] for i, e in enumerate(gt_multi_hot_label) if e]\n",
    "                if img_id not in my_anno_dict.keys():\n",
    "                    my_anno_dict[img_id] = {\n",
    "                        \"obj\": [gt_cat],\n",
    "                        \"coord\": [gt_coord]\n",
    "                    }           \n",
    "                else:\n",
    "                    my_anno_dict[img_id][\"obj\"].append(gt_cat)\n",
    "                    my_anno_dict[img_id][\"coord\"].append(gt_coord) \n",
    "\n",
    "\n",
    "baseline_detection = './baseline_res/{}.txt'\n",
    "baseline_gt = './baseline_res/GT_{}.txt'\n",
    "\n",
    "baseline_query_logits = []\n",
    "for i in range(8):\n",
    "    baseline_detection_ = baseline_detection.format(i)\n",
    "    with open(baseline_detection_) as f:\n",
    "        for line in f.readlines():\n",
    "            img_id = line.split(' ')[0]\n",
    "            if key_frame != img_id:\n",
    "                continue\n",
    "            else:\n",
    "                annotation = [float(n) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "                # multi_hot_obj_label = [int(n) for n in annotation[6:]]\n",
    "                baseline_query_logits.append(annotation)\n",
    "\n",
    "baseline_anno_dict = {}\n",
    "for i in range(8):\n",
    "    baseline_gt_ = baseline_gt.format(i)\n",
    "    with open(baseline_gt_) as f:\n",
    "        for line in f.readlines():\n",
    "            img_id = line.split(' ')[0]\n",
    "            if img_id != key_frame:\n",
    "                continue\n",
    "            else:\n",
    "                annotation = [int(float(n)) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "                gt_coord = annotation[2:6]\n",
    "                # gtxmin, gtymin, gtxmax, gtymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "                gt_multi_hot_label = annotation[6:]\n",
    "                gt_cat = [items[i+1] for i, e in enumerate(gt_multi_hot_label) if e]\n",
    "                if img_id not in baseline_anno_dict.keys():\n",
    "                    baseline_anno_dict[img_id] = {\n",
    "                        \"obj\": [gt_cat],\n",
    "                        \"coord\": [gt_coord]\n",
    "                    }           \n",
    "                else:\n",
    "                    baseline_anno_dict[img_id][\"obj\"].append(gt_cat)\n",
    "                    baseline_anno_dict[img_id][\"coord\"].append(gt_coord) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualization_utils_custom as vis_utils\n",
    "img_show = orig_vid[16]\n",
    "for _, (obj, gt_coord) in enumerate(zip(my_anno_dict[key_frame][\"obj\"], my_anno_dict[key_frame][\"coord\"])):\n",
    "    gt_cat = str(obj)\n",
    "    gt_xmin, gt_ymin, gt_xmax, gt_ymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "    vis_utils.draw_bounding_box_on_image(\n",
    "        img_show, gt_ymin, gt_xmin, gt_ymax, gt_xmax,\n",
    "        color = 'Green',\n",
    "        display_str_list=[gt_cat],\n",
    "        use_normalized_coordinates=False,\n",
    "        margin2=20\n",
    "    )\n",
    "    print(gt_cat)\n",
    "img_show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of decoder query attention (of the second transformer)\n",
    "(Simpler version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import visualization_utils_custom as vis_utils\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.image as mpimg\n",
    "import imageio.v2 as imageio\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "%matplotlib inline\n",
    "# curr_weights = (curr_cls_dec_attn_weights + curr_dec_attn_weights)[0]\n",
    "# curr_weights = (curr_dec_attn_weights)[0]\n",
    "curr_weights = attn\n",
    "class_of_interest = 73\n",
    "\n",
    "h2, w2 = curr_conv_features.shape[-2:]\n",
    "# curr_weights = curr_weights.transpose(0,1).reshape(1, 15, 32, h2, w2)\n",
    "# curr_weights = curr_weights.view(1,15,32,h2,w2)\n",
    "fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(h,w//3+1))\n",
    "ims = []\n",
    "t = 16\n",
    "\n",
    "target_img = orig_vid[t]\n",
    "fig.suptitle(\"current model (time {})\".format(t) + \" class: {}\".format(items[class_of_interest+1]))\n",
    "\n",
    "for i, ax_i in enumerate(axs):\n",
    "    for j in range(5):\n",
    "        tgt_img = copy.deepcopy(target_img)\n",
    "        # coord = my_query_logits[i*5+j][:4]\n",
    "        coord = box_cxcywh_to_xyxy(my_outputs[\"pred_boxes\"][0, i*5+j])\n",
    "        # logits = my_query_logits[i*5+j][4:-1]\n",
    "        logits = my_outputs[\"pred_logits\"][0, i*5+j].sigmoid() * my_outputs[\"pred_logits_b\"].softmax(-1)[0, i*5+j, 1:2]\n",
    "        bxmin, bymin, bxmax, bymax = coord[0], coord[1], coord[2], coord[3]\n",
    "        cat = [items[i+1] for i, e in enumerate([k>0.4 for k in logits]) if e]\n",
    "        ax_i[j].set_title(\"query {}\".format(i*5+j))\n",
    "        if len(cat) == 0:                \n",
    "            pass\n",
    "        elif t == 16:\n",
    "            # for _, (obj, gt_coord) in enumerate(zip(my_anno_dict[key_frame][\"obj\"], my_anno_dict[key_frame][\"coord\"])):\n",
    "            #     gt_cat = str(obj)\n",
    "            #     gt_xmin, gt_ymin, gt_xmax, gt_ymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "            #     vis_utils.draw_bounding_box_on_image(\n",
    "            #         tgt_img, gt_ymin, gt_xmin, gt_ymax, gt_xmax,\n",
    "            #         color = 'Green',\n",
    "            #         display_str_list=[gt_cat],\n",
    "            #         use_normalized_coordinates=False,\n",
    "            #         margin2=20\n",
    "            #     )\n",
    "            #     print(gt_cat)   \n",
    "            vis_utils.draw_bounding_box_on_image(\n",
    "                tgt_img, bymin, bxmin, bymax, bxmax,\n",
    "                    color = 'Yellow',\n",
    "                    display_str_list=cat,\n",
    "                    # use_normalized_coordinates=False,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    margin2=30\n",
    "                )\n",
    "            \n",
    "            print(cat)\n",
    "                \n",
    "        else:\n",
    "            pass\n",
    "        ax_i[j].imshow(tgt_img)\n",
    "        ax_i[j].imshow(attn[-1][i*5+j, class_of_interest].detach().cpu().view(h, w), cmap='seismic', interpolation='bicubic', alpha=.5, extent=(xmin, xmax, ymin, ymax))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_outputs[\"pred_logits_b\"].softmax(-1)[:, :, 1:2] # 1, 15, 1\n",
    "my_outputs[\"pred_logits\"][0, :, 41].sigmoid()\n",
    "# my_outputs[\"pred_boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import visualization_utils_custom as vis_utils\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.image as mpimg\n",
    "import imageio.v2 as imageio\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_along_(l):\n",
    "    baseline_weights = baseline_dec_attn_weights[5-l]\n",
    "    h2, w2 = baseline_conv_features.shape[-2:]\n",
    "    baseline_weights = baseline_weights.reshape(1, 15, 1, h2, w2)\n",
    "    baseline_weights = baseline_weights.view(1,15,1,h2,w2)\n",
    "    fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(h,w//3+1))\n",
    "    t = 16\n",
    "    \n",
    "    target_img = orig_vid[t]\n",
    "    fig.suptitle(\"baseline (layer {})\".format(l))\n",
    "\n",
    "    for i, ax_i in enumerate(axs):\n",
    "        for j in range(5):\n",
    "            tgt_img = copy.deepcopy(target_img)\n",
    "            coord = baseline_query_logits[i*5+j][:4]\n",
    "            logits = baseline_query_logits[i*5+j][4:-1]\n",
    "            bxmin, bymin, bxmax, bymax = coord[0], coord[1], coord[2], coord[3]\n",
    "            cat = [items[i+1] for i, e in enumerate([k>0.4 for k in logits]) if e]\n",
    "            ax_i[j].set_title(\"query {}\".format(i*5+j))\n",
    "            if len(cat) == 0:                \n",
    "                continue\n",
    "            elif t == 16:\n",
    "                # for _, (obj, gt_coord) in enumerate(zip(baseline_anno_dict[key_frame][\"obj\"], baseline_anno_dict[key_frame][\"coord\"])):\n",
    "                #     gt_cat = str(obj)\n",
    "                #     gt_xmin, gt_ymin, gt_xmax, gt_ymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "                #     vis_utils.draw_bounding_box_on_image(\n",
    "                #         tgt_img, gt_ymin, gt_xmin, gt_ymax, gt_xmax,\n",
    "                #         color = 'Green',\n",
    "                #         display_str_list=[gt_cat],\n",
    "                #         use_normalized_coordinates=False,\n",
    "                #         margin2=20\n",
    "                #     )   \n",
    "                vis_utils.draw_bounding_box_on_image(\n",
    "                    tgt_img, bymin, bxmin, bymax, bxmax,\n",
    "                        color = 'Yellow',\n",
    "                        display_str_list=cat,\n",
    "                        use_normalized_coordinates=False,\n",
    "                        margin2=30\n",
    "                    )            \n",
    "            else:\n",
    "                pass\n",
    "            ax_i[j].imshow(tgt_img)\n",
    "            ax_i[j].imshow(baseline_weights[0, i*5+j, 0].detach().cpu().view(h, w), cmap='seismic', interpolation='bicubic', alpha=.6, extent=(xmin, xmax, ymin, ymax))\n",
    "    fig.savefig('./temp.png')\n",
    "    img = imageio.imread('./temp.png')\n",
    "    return img\n",
    "\n",
    "ims = []\n",
    "for l in range(6):\n",
    "    ims.append(plot_along_(l))\n",
    "\n",
    "imageio.mimsave('./temp.gif', ims, duration=0.3)\n",
    "\n",
    "display.Image(\"./temp.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_query_logits.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import visualization_utils_custom as vis_utils\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.image as mpimg\n",
    "import imageio.v2 as imageio\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_along_(l):\n",
    "    # curr_weights = (curr_cls_dec_attn_weights + curr_dec_attn_weights)[5-l]\n",
    "    # curr_weights = curr_cls_dec_attn_weights[5-l]\n",
    "    curr_weights = attn[l] # 15, 1, 16, 25\n",
    "    h2, w2 = curr_conv_features.shape[-2:]\n",
    "    # curr_weights = curr_weights.transpose(0,1).reshape(1, 15, 32, h2, w2)\n",
    "    # curr_weights = curr_weights.view(1,15,32,h2,w2)\n",
    "    fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(h,w//3+1))\n",
    "    # t = 16\n",
    "    class_of_interest = 11\n",
    "    target_img = orig_vid[t]\n",
    "    fig.suptitle(\"current model (layer {}), label: {}\".format(l, items[class_of_interest+1]))\n",
    "\n",
    "    for i, ax_i in enumerate(axs):\n",
    "        for j in range(5):\n",
    "            tgt_img = copy.deepcopy(target_img)\n",
    "            coord = my_query_logits[i*5+j][:4]\n",
    "            logits = my_query_logits[i*5+j][4:-1]\n",
    "            bxmin, bymin, bxmax, bymax = coord[0], coord[1], coord[2], coord[3]\n",
    "            cat = [items[i+1] for i, e in enumerate([k>0.5 for k in logits]) if e]\n",
    "            ax_i[j].set_title(\"query {}\".format(i*5+j))\n",
    "            if len(cat) == 0:                \n",
    "                continue\n",
    "            elif t == 16:\n",
    "                # for _, (obj, gt_coord) in enumerate(zip(my_anno_dict[key_frame][\"obj\"], my_anno_dict[key_frame][\"coord\"])):\n",
    "                #     gt_cat = str(obj)\n",
    "                #     gt_xmin, gt_ymin, gt_xmax, gt_ymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "                #     vis_utils.draw_bounding_box_on_image(\n",
    "                #         tgt_img, gt_ymin, gt_xmin, gt_ymax, gt_xmax,\n",
    "                #         color = 'Green',\n",
    "                #         display_str_list=[gt_cat],\n",
    "                #         use_normalized_coordinates=False,\n",
    "                #         margin2=20\n",
    "                #     )   \n",
    "                vis_utils.draw_bounding_box_on_image(\n",
    "                    tgt_img, bymin, bxmin, bymax, bxmax,\n",
    "                        color = 'Yellow',\n",
    "                        display_str_list=cat,\n",
    "                        use_normalized_coordinates=False,\n",
    "                        margin2=30\n",
    "                    )            \n",
    "            else:\n",
    "                pass\n",
    "            ax_i[j].imshow(tgt_img)\n",
    "            ax_i[j].imshow(curr_weights[i*5+j, class_of_interest].detach().cpu().view(h, w), cmap='seismic', interpolation='bicubic', alpha=.6, extent=(xmin, xmax, ymin, ymax))\n",
    "    fig.savefig('./temp.png')\n",
    "    img = imageio.imread('./temp.png')\n",
    "    return img\n",
    "\n",
    "ims = []\n",
    "for l in range(6):\n",
    "    ims.append(plot_along_(l))\n",
    "\n",
    "imageio.mimsave('./temp.gif', ims, duration=0.3)\n",
    "\n",
    "display.Image(\"./temp.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import visualization_utils_custom as vis_utils\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.image as mpimg\n",
    "import imageio.v2 as imageio\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "baseline_weights = baseline_dec_attn_weights\n",
    "h2, w2 = baseline_conv_features.shape[-2:]\n",
    "baseline_weights = baseline_weights.reshape(1, 15, 1, h2, w2)\n",
    "baseline_weights = baseline_weights.view(1,15,1,h2,w2)\n",
    "fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(h,w//3+1))\n",
    "ims = []\n",
    "t = 16\n",
    "\n",
    "target_img = orig_vid[t]\n",
    "fig.suptitle(\"baseline (time {})\".format(t))\n",
    "\n",
    "for i, ax_i in enumerate(axs):\n",
    "    for j in range(5):\n",
    "        tgt_img = copy.deepcopy(target_img)\n",
    "        coord = baseline_query_logits[i*5+j][:4]\n",
    "        logits = baseline_query_logits[i*5+j][4:-1]\n",
    "        bxmin, bymin, bxmax, bymax = coord[0], coord[1], coord[2], coord[3]\n",
    "        cat = [items[i+1] for i, e in enumerate([k>0.4 for k in logits]) if e]\n",
    "        ax_i[j].set_title(\"query {}\".format(i*5+j))\n",
    "        if len(cat) == 0:                \n",
    "            continue\n",
    "        elif t == 16:\n",
    "            for _, (obj, gt_coord) in enumerate(zip(baseline_anno_dict[key_frame][\"obj\"], baseline_anno_dict[key_frame][\"coord\"])):\n",
    "                gt_cat = str(obj)\n",
    "                gt_xmin, gt_ymin, gt_xmax, gt_ymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "                vis_utils.draw_bounding_box_on_image(\n",
    "                    tgt_img, gt_ymin, gt_xmin, gt_ymax, gt_xmax,\n",
    "                    color = 'Green',\n",
    "                    display_str_list=[gt_cat],\n",
    "                    use_normalized_coordinates=False,\n",
    "                    margin2=20\n",
    "                )   \n",
    "            vis_utils.draw_bounding_box_on_image(\n",
    "                tgt_img, bymin, bxmin, bymax, bxmax,\n",
    "                    color = 'Yellow',\n",
    "                    display_str_list=cat,\n",
    "                    use_normalized_coordinates=False,\n",
    "                    margin2=30\n",
    "                )            \n",
    "        else:\n",
    "            pass\n",
    "        ax_i[j].imshow(tgt_img)\n",
    "        ax_i[j].imshow(baseline_weights[0, i*5+j, 0].detach().cpu().view(h, w), cmap='copper', interpolation='bicubic', alpha=.6, extent=(xmin, xmax, ymin, ymax))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import visualization_utils_custom as vis_utils\n",
    "\n",
    "%matplotlib inline\n",
    "# classification decoder weight plotting along time axis\n",
    "fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(h,w//3+1))\n",
    "ims = []\n",
    "target_img = orig_vid[16]\n",
    "t = 2\n",
    "fig.suptitle(\"time {}\".format(t))\n",
    "dec_attn_weights_ = sum(dec_attn_weights)\n",
    "\n",
    "for i, ax_i in enumerate(axs):\n",
    "    for j in range(5):\n",
    "\n",
    "        tgt_img = copy.deepcopy(target_img)\n",
    "        coord = query_logits[i*5+j][:4]\n",
    "        logits = query_logits[i*5+j][4:-1]\n",
    "        bxmin, bymin, bxmax, bymax = coord[0], coord[1], coord[2], coord[3]\n",
    "        cat = [items[i+1] for i, e in enumerate([k>0.4 for k in logits]) if e]\n",
    "        if len(cat) != 0:\n",
    "            for _, (obj, gt_coord) in enumerate(zip(anno_dict[key_frame][\"obj\"], anno_dict[key_frame][\"coord\"])):\n",
    "                gt_cat = str(obj)\n",
    "                gt_xmin, gt_ymin, gt_xmax, gt_ymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "                vis_utils.draw_bounding_box_on_image(\n",
    "                    tgt_img, gt_ymin, gt_xmin, gt_ymax, gt_xmax,\n",
    "                    color = 'Green',\n",
    "                    display_str_list=[gt_cat],\n",
    "                    use_normalized_coordinates=False,\n",
    "                    margin2=20\n",
    "                )\n",
    "            vis_utils.draw_bounding_box_on_image(\n",
    "                tgt_img, bymin, bxmin, bymax, bxmax,\n",
    "                    color = 'Yellow',\n",
    "                    display_str_list=cat,\n",
    "                    use_normalized_coordinates=False,\n",
    "                    margin2=30\n",
    "                )\n",
    "\n",
    "            ax_i[j].imshow(tgt_img)\n",
    "            ax_i[j].imshow(dec_attn_weights_[0, i*5+j].detach().cpu().view(h, w), cmap='copper', interpolation='bicubic', alpha=.6, extent=(xmin, xmax, ymin, ymax))\n",
    "        ax_i[j].set_title(\"query {}\".format(i*5+j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import visualization_utils_custom as vis_utils\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.image as mpimg\n",
    "import imageio.v2 as imageio\n",
    "from IPython import display\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "def plot_along_(l):\n",
    "    dec_attn_weights_ = dec_attn_weights[5-l]\n",
    "    h2, w2 = conv_features.shape[-2:]\n",
    "    dec_attn_weights_ = dec_attn_weights_.reshape(1, 15, 1, h2, w2)\n",
    "    dec_attn_weights_ = dec_attn_weights_.view(1,15,1,h2,w2)\n",
    "    fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(h,w//3+1))\n",
    "    t = 16\n",
    "\n",
    "    target_img = orig_vid[t]\n",
    "    fig.suptitle(\"tuber (layer {})\".format(l))\n",
    "\n",
    "    for i, ax_i in enumerate(axs):\n",
    "        for j in range(5):\n",
    "            tgt_img = copy.deepcopy(target_img)\n",
    "            coord = query_logits[i*5+j][:4]\n",
    "            logits = query_logits[i*5+j][4:-1]\n",
    "            bxmin, bymin, bxmax, bymax = coord[0], coord[1], coord[2], coord[3]\n",
    "            cat = [items[i+1] for i, e in enumerate([k>0.4 for k in logits]) if e]\n",
    "            ax_i[j].set_title(\"query {}\".format(i*5+j))\n",
    "            if len(cat) == 0:                \n",
    "                continue\n",
    "            elif t == 16:\n",
    "                for _, (obj, gt_coord) in enumerate(zip(anno_dict[key_frame][\"obj\"], anno_dict[key_frame][\"coord\"])):\n",
    "                    gt_cat = str(obj)\n",
    "                    gt_xmin, gt_ymin, gt_xmax, gt_ymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "                    vis_utils.draw_bounding_box_on_image(\n",
    "                        tgt_img, gt_ymin, gt_xmin, gt_ymax, gt_xmax,\n",
    "                        color = 'Green',\n",
    "                        display_str_list=[gt_cat],\n",
    "                        use_normalized_coordinates=False,\n",
    "                        margin2=20\n",
    "                    )   \n",
    "                vis_utils.draw_bounding_box_on_image(\n",
    "                    tgt_img, bymin, bxmin, bymax, bxmax,\n",
    "                        color = 'Yellow',\n",
    "                        display_str_list=cat,\n",
    "                        use_normalized_coordinates=False,\n",
    "                        margin2=30\n",
    "                    )            \n",
    "            else:\n",
    "                pass\n",
    "            ax_i[j].imshow(tgt_img)\n",
    "            ax_i[j].imshow(dec_attn_weights_[0, i*5+j].detach().cpu().view(h, w), cmap='copper', interpolation='bicubic', alpha=.6, extent=(xmin, xmax, ymin, ymax))\n",
    "    fig.savefig('./temp.png')\n",
    "    img = imageio.imread('./temp.png')\n",
    "    return img\n",
    "\n",
    "ims = []\n",
    "for l in range(6):\n",
    "    ims.append(plot_along_(l))\n",
    "\n",
    "imageio.mimsave('./temp.gif', ims, duration=0.3)\n",
    "\n",
    "display.Image(\"./temp.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dec_attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_attn_weights.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
